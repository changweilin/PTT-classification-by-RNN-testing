{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1983,
     "status": "ok",
     "timestamp": 1517421478061,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "XLMfGvYgCdF6",
    "outputId": "46e00917-e97b-4ae8-a609-6c808458e91b"
   },
   "outputs": [],
   "source": [
    "# Classify PTT (Tech_job broad) posts to content type by RNN model.\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import types \n",
    "\n",
    "#Initialize for files name and path.\n",
    "base_dir = 'C:/Users/User/Raw data/PTT'\n",
    "board = 'Tech_job'\n",
    "now_time = '1519748408.0598423'\n",
    "board_time = board + '_' + now_time\n",
    "\n",
    "board_dir = os.path.join(base_dir, board_time)\n",
    "if not os.path.exists(board_dir):\n",
    "    os.makedirs(board_dir)\n",
    "content_dir = os.path.join(board_dir, 'content')\n",
    "if not os.path.exists(content_dir):\n",
    "    os.makedirs(content_dir)\n",
    "push_dir = os.path.join(board_dir, 'push')\n",
    "if not os.path.exists(push_dir):\n",
    "    os.makedirs(push_dir)\n",
    "\n",
    "type_label = ['討論', '心得', '請益', '面試', '聘書', '徵才', '新聞', '情報']\n",
    "type_newlabel = ['討論', '心得', '請益', '情報']\n",
    "label_num = len(type_label)\n",
    "label_newnum = len(type_newlabel)\n",
    "\n",
    "def type2number(type_word):\n",
    "    type_num = 0\n",
    "    for type_name in type_newlabel:\n",
    "        if type_word.find(type_name)>=0:\n",
    "            break\n",
    "        type_num = type_num + 1\n",
    "    return type_num\n",
    "\n",
    "def symbol_filter(crazystring):\n",
    "    filter_words=' !\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'　，。！：；、？﹝﹞「」『』（）｛｝［］【】《》“”‘’＼｜〝〞‵′＋－＊／＝≦≧＿＠＃＄％︿＆～§◎．※ㄧ↔│○●☆★◇◆□■▽▼△▲㊣⊙⊕ˍ…﹌﹋﹎﹍﹉﹊‥–↑↓←→↖↗↙↘∥∕℅≒≡∩∪∞￣＿◤◥◣◢∵∴〒⊥∠⊿┼┴┬┤├▔─│▕┌┐└┘╭╮╰╯═╞╪╡╔╦╗╠╬╣╚╩╝╒╤╕╘╧╛╓╥╖╟╫╢╙╨╜║▓╱╲╳▁▂▄▅▆▇█▏▎▍▌▋▊▉▁▔'\n",
    "    is_filtered = True\n",
    "    for crazychar in crazystring:\n",
    "        if filter_words.find(crazychar)<0:\n",
    "            is_filtered = False\n",
    "    return is_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  1\n",
      "0.1:  91.0\n",
      "0.25:  146.0\n",
      "Mean:  550.8913021345912\n",
      "0.75:  698.0\n",
      "0.9:  1181.0\n",
      "0.95:  1674.0\n",
      "Max:  19644\n",
      "Std:  728.991418870587\n",
      "\r",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1084713565.A.D36.html 職位中英文\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1143282919.A.F48.html [就可] 嫁給工程師的好處多?! ^_^\n",
      "\r",
      "\r",
      "\r",
      "\r",
      "無關 :  https://www.ptt.cc/bbs/Tech_Job/M.1220011862.A.651.html boylover兄臺\n",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "檢舉 :  https://www.ptt.cc/bbs/Tech_Job/M.1249658939.A.5D9.html  鬧板 \n",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "申請 :  https://www.ptt.cc/bbs/Tech_Job/M.1255582856.A.ADA.html  Tech_Job板申請新增板主\n",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "論卦 :  https://www.ptt.cc/bbs/Tech_Job/M.1259327542.A.ACA.html  通膨經濟學、兩萬二、與台灣人大滅絕\n",
      "建議 :  https://www.ptt.cc/bbs/Tech_Job/M.1259482668.A.A6E.html 精華區\"公司\"部分\n",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "轉文 :  https://www.ptt.cc/bbs/Tech_Job/M.1270320846.A.2CF.html"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 派遣工作 缺點(轉)  \n",
      "轉潑 :  https://www.ptt.cc/bbs/Tech_Job/M.1270482835.A.038.html  職場人都該好好的看這一篇文章\n",
      "轉潑 :  https://www.ptt.cc/bbs/Tech_Job/M.1270483282.A.862.html 職場人都該好好的看這一篇文章[2]\n",
      "好文 :  https://www.ptt.cc/bbs/Tech_Job/M.1270490410.A.01A.html  請把我們當人看-ㄧ個洋華員工的心聲\n",
      "申請 :  https://www.ptt.cc/bbs/Tech_Job/M.1314891379.A.5B9.html  Tech_Job 板發文限制申請\n",
      "申請 :  https://www.ptt.cc/bbs/Tech_Job/M.1318867663.A.EDC.html  Tech_Job板申請新增板主\n",
      "申請 :  https://www.ptt.cc/bbs/Tech_Job/M.1321711030.A.59D.html  Tech_Job新增板主mmkntust\n",
      "新增 :  https://www.ptt.cc/bbs/Tech_Job/M.1325561824.A.F07.html  Tech_Job 新增板主 mmkntust/harrygp/lovewsc\n",
      "猶豫 :  https://www.ptt.cc/bbs/Tech_Job/M.1340671920.A.E57.html  工程師？還是國考？\n",
      "非關科技版 :  https://www.ptt.cc/bbs/Tech_Job/M.1342092631.A.181.html  風力+太陽能 並聯\n",
      "週刊 :  https://www.ptt.cc/bbs/Tech_Job/M.1342684240.A.12B.html  台灣公務員與死老百姓的收入財富比較\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1345795739.A.A35.html 外商來台設立研發中心, 還要政府補助? ꨠ\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1345820997.A.AD9.html 外商來台設立研發中心, 還要政府補助?  \n",
      "抱怨 :  https://www.ptt.cc/bbs/Tech_Job/M.1345835067.A.E74.html  沒那個屁股就別吃那個瀉藥...\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1345921116.A.8EB.html 外商來台設立研發中心, 還要政府補助?  \n",
      "難過 :  https://www.ptt.cc/bbs/Tech_Job/M.1346081731.A.F13.html  尋找車禍目擊者或行經車輛\n",
      "難過 :  https://www.ptt.cc/bbs/Tech_Job/M.1346202595.A.EA3.html  尋找車禍目擊者或行經車輛\n",
      "假聞 :  https://www.ptt.cc/bbs/Tech_Job/M.1346394669.A.0CC.html  甜短壽：年輕人缺乏熱忱使命感\n",
      "售書 :  https://www.ptt.cc/bbs/Tech_Job/M.1347438641.A.285.html  白皮書、深入淺出Java、SQL等..五本經典~\n",
      "求職 :  https://www.ptt.cc/bbs/Tech_Job/M.1347688794.A.A0B.html  作業員月薪4萬?\n",
      "影片 :  https://www.ptt.cc/bbs/Tech_Job/M.1347903734.A.FF8.html  友達公司的簡介\n",
      "影片 :  https://www.ptt.cc/bbs/Tech_Job/M.1347932845.A.F54.html  友達公司的簡介\n",
      "代PO :  https://www.ptt.cc/bbs/Tech_Job/M.1349572757.A.6A0.html  和碩聯合科技 研發替代役說明會(交大場)\n",
      "文章 :  https://www.ptt.cc/bbs/Tech_Job/M.1350406219.A.864.html 勞工陳情 主委落跑 工會意見未獲回應 \n",
      "網宣 :  https://www.ptt.cc/bbs/Tech_Job/M.1350821874.A.E59.html 2012 馬達設計虛擬測試平台暨優化分析研討\n",
      "代PO :  https://www.ptt.cc/bbs/Tech_Job/M.1351167627.A.681.html  和碩聯合科技 研發替代役說明會 成大場\n",
      "轉職 :  https://www.ptt.cc/bbs/Tech_Job/M.1362152270.A.CE3.html  該選擇離開還是留下來?\n",
      "網宣 :  https://www.ptt.cc/bbs/Tech_Job/M.1363059525.A.FC6.html  Google與nVIDIA搶搭的雲端遊戲技術\n",
      "轉載 :  https://www.ptt.cc/bbs/Tech_Job/M.1364151514.A.4C0.html  我在 Google 的第一課 by 翟本喬 博士\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1364241633.A.D89.html  矽谷創投看台灣年輕人\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1364283321.A.B40.html  矽谷創投看台灣年輕人\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1364309709.A.DF1.html  矽谷創投看台灣年輕人\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1364312258.A.C4F.html  矽谷創投看台灣年輕人\n",
      "轉貼 :  https://www.ptt.cc/bbs/Tech_Job/M.1365220183.A.C49.html  用力工作十五年\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1367497472.A.CB2.html  爸爸的薪水\n",
      "職場 :  https://www.ptt.cc/bbs/Tech_Job/M.1369060737.A.6AC.html  孕婦的悲哀\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1369888670.A.D9B.html  大敗局：被新技術“殺”死的大公司\n",
      "標的 :  https://www.ptt.cc/bbs/Tech_Job/M.1373605455.A.68A.html  聯發科營收成長，EPS卻沒跟上?\n",
      "懇請幫助 :  https://www.ptt.cc/bbs/Tech_Job/M.1377433648.A.837.html  我在組務發起反對網路創業版與創業版合併連署\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1378647231.A.3A0.html  高通放出影片嗆聲聯發科「真八核」：量\n",
      "影音 :  https://www.ptt.cc/bbs/Tech_Job/M.1387594659.A.724.html  韓國人花大錢補習 搶進三星企業\n",
      "生涯 :  https://www.ptt.cc/bbs/Tech_Job/M.1394612441.A.31D.html  想試著走品質管理(品保)路線該如何學習\n",
      "囧rz :  https://www.ptt.cc/bbs/Tech_Job/M.1396365559.A.8C9.html  用綠和透明墨水畫出七條互相垂直的紅線\n",
      "生活 :  https://www.ptt.cc/bbs/Tech_Job/M.1397637110.A.235.html  善化菜市場快閃!!!\n",
      "網宣 :  https://www.ptt.cc/bbs/Tech_Job/M.1399308309.A.8BD.html   電資工會 X 台大工會  職場肝苦談\n",
      "轉讓 :  https://www.ptt.cc/bbs/Tech_Job/M.1402483821.A.D05.html  竹南科學園區旁/中華路上套房轉讓\n",
      "存檔 :  https://www.ptt.cc/bbs/Tech_Job/M.1410417088.A.E0D.html  緯創     薪水\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1410887933.A.B03.html  雲巖科技及揚鵬機械相關資訊\n",
      "問券 :  https://www.ptt.cc/bbs/Tech_Job/M.1415345234.A.C40.html 請大家救救研究生，幫忙寫一下問券\n",
      "徵 :  https://www.ptt.cc/bbs/Tech_Job/M.1420552234.A.73B.html  台積客製手機\n",
      "抽禮卷 :  https://www.ptt.cc/bbs/Tech_Job/M.1420724406.A.580.html 線上支持團體(工作壓力)使用行為探討\n",
      "北京 :  https://www.ptt.cc/bbs/Tech_Job/M.1422959959.A.AF2.html [IT網路][工程師]一搭科技徵創業夥伴\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1423499344.A.283.html  和碩科技\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1425665901.A.405.html  同和國際\n",
      "亞洲 :  https://www.ptt.cc/bbs/Tech_Job/M.1427537965.A.2C7.html  工作選擇,選賺錢多,還是去前途好?\n",
      "選擇 :  https://www.ptt.cc/bbs/Tech_Job/M.1428987863.A.DB2.html  華碩SPM or 廣達PM\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1429809116.A.4A8.html  台灣福興工業\n",
      "選擇 :  https://www.ptt.cc/bbs/Tech_Job/M.1430153107.A.011.html  轉換跑道或繼續原公司...\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1430827869.A.FD3.html  永豐餘\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1430872293.A.7CD.html  永豐餘\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1431011286.A.FAA.html  永豐餘\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1431512534.A.657.html  台灣鐵三角(中壢)\n",
      "標的 :  https://www.ptt.cc/bbs/Tech_Job/M.1431919932.A.36F.html  公司離職率排名\n",
      "記名公投 :  https://www.ptt.cc/bbs/Tech_Job/M.1432120587.A.BD1.html  包子\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1432200130.A.C68.html  台灣鐵三角(中壢)\n",
      "笑話 :  https://www.ptt.cc/bbs/Tech_Job/M.1433070442.A.5AB.html  Kobe的成功\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434551070.A.D0C.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434608206.A.6FC.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434634454.A.B8F.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434689378.A.135.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434700491.A.2A6.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434707838.A.A09.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434778039.A.567.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434824227.A.933.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434857213.A.12B.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434887813.A.B54.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434899564.A.84F.html  清大教授彭明輝：台積電是二流企業\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1434902291.A.373.html  清大教授彭明輝：台積電是二流企業\n",
      "新增 :  https://www.ptt.cc/bbs/Tech_Job/M.1436100296.A.5C3.html  Tech_Job板 新增板主一位 staff23\n",
      "新增 :  https://www.ptt.cc/bbs/Tech_Job/M.1438273743.A.D40.html  Tech_Job板 新增板主一位 joh\n",
      "職場 :  https://www.ptt.cc/bbs/Tech_Job/M.1440338509.A.EDA.html  轉貼_2015最全500強應屆生薪資表\n",
      "好物 :  https://www.ptt.cc/bbs/Tech_Job/M.1441167349.A.0C8.html  短片及動畫創作比賽！\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1444723330.A.BB5.html  榮益科技股份有限公司\n",
      "舊聞 :  https://www.ptt.cc/bbs/Tech_Job/M.1448160410.A.F92.html  勞基法修正 假日兩倍薪，七日內還有一天\n",
      "遺失 :  https://www.ptt.cc/bbs/Tech_Job/M.1449492709.A.8E1.html 台積手機遺失在金山街7-11附近@12/8 20:20\n",
      "問券 :  https://www.ptt.cc/bbs/Tech_Job/M.1451812024.A.E99.html  行動用戶攜碼離網議題(抽禮券贈品)\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1452011046.A.191.html 領多少年終、基本工資才合理？\n",
      "ＦＢ :  https://www.ptt.cc/bbs/Tech_Job/M.1452513856.A.58D.html  謝金河-紫光這下子被「熔斷」了！\n",
      "戰友 :  https://www.ptt.cc/bbs/Tech_Job/M.1454687186.A.4EF.html  [全國]openGL 計算機圖學 影像處理\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1455777437.A.5F2.html  天崗資訊股份有限公司\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1463493226.A.2D4.html  Java的感化讓我從酒店少爺到軟體工程師\n",
      "拾獲 :  https://www.ptt.cc/bbs/Tech_Job/M.1466213766.A.B3A.html  工作證一張\n",
      "ＦＢ :  https://www.ptt.cc/bbs/Tech_Job/M.1466522060.A.295.html  桃園空服員工會：同意罷工\n",
      "回役 :  https://www.ptt.cc/bbs/Tech_Job/M.1466746355.A.B55.html  幸福王國回憶錄-序\n",
      "回役 :  https://www.ptt.cc/bbs/Tech_Job/M.1466749173.A.8D1.html  幸福王國回憶錄-1\n",
      "ＦＢ :  https://www.ptt.cc/bbs/Tech_Job/M.1467597343.A.7E3.html  求職天眼通--更新[顯示違反勞基法雇主]\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1469814133.A.916.html  淪落到只能靠削價競爭?\n",
      "存檔 :  https://www.ptt.cc/bbs/Tech_Job/M.1470110212.A.9AD.html  某三線代工快變四線的     薪水\n",
      "共乘 :  https://www.ptt.cc/bbs/Tech_Job/M.1474117252.A.377.html  上班日 淡水新市鎮=>內湖科學園區(西湖)\n",
      "免費 :  https://www.ptt.cc/bbs/Tech_Job/M.1476442719.A.187.html 工作人生顧問\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1483511313.A.AAA.html  日立化成能源-產品專案管理人員\n",
      "代ｐｏ :  https://www.ptt.cc/bbs/Tech_Job/M.1484120155.A.C38.html 外商金士頓這種是不是很照顧員工？\n",
      "代ｐｏ :  https://www.ptt.cc/bbs/Tech_Job/M.1484451624.A.912.html 外商金士頓這種是不是很照顧員工？\n",
      "代ｐｏ :  https://www.ptt.cc/bbs/Tech_Job/M.1484463124.A.4E4.html 外商金士頓這種是不是很照顧員工？\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1485135400.A.06D.html  星博電子(高雄)行銷專員?\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1488300599.A.3E2.html  六和化工\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "免費 :  https://www.ptt.cc/bbs/Tech_Job/M.1488777506.A.245.html 工作人生顧問\n",
      "ＦＢ :  https://www.ptt.cc/bbs/Tech_Job/M.1488857447.A.24A.html  中華民國行政院FB:中台灣機械之都啟航\n",
      "舊聞 :  https://www.ptt.cc/bbs/Tech_Job/M.1489677705.A.676.html  下班傳LINE算加班 資方痛批：「刁勞」越\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1490262082.A.661.html  大船集團-達航科技\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1490357939.A.F46.html  大船集團-達航科技\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1492788084.A.060.html  群燿科技\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1494545379.A.110.html  豐軒科技股份有限公司\n",
      "ＦＢ :  https://www.ptt.cc/bbs/Tech_Job/M.1496071077.A.1BE.html  時力:數位金融要發展 就從告別財金幫開始\n",
      "ＦＢ :  https://www.ptt.cc/bbs/Tech_Job/M.1496453160.A.228.html  余宛如-開放外國白領來台才能接軌國際\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1496712119.A.242.html  前瞻計畫為啥沒遊戲產業?\n",
      "問券 :  https://www.ptt.cc/bbs/Tech_Job/M.1498810356.A.F7D.html 救救研究生 順便抽餐券\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1499303211.A.F8C.html  機能衣大躍進，紡織矽谷的崛起\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1500953352.A.EDA.html  圓凱科技實業\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1502157019.A.EA6.html  三合微科技\n",
      "公司 :  https://www.ptt.cc/bbs/Tech_Job/M.1505748637.A.B48.html  合盈光電\n",
      "FB :  https://www.ptt.cc/bbs/Tech_Job/M.1505965035.A.948.html 求職天眼通宣佈結束公司營運\n",
      "FB :  https://www.ptt.cc/bbs/Tech_Job/M.1506356128.A.027.html 求職天眼通宣佈結束公司營運\n",
      "轉錄 :  https://www.ptt.cc/bbs/Tech_Job/M.1507647143.A.E6E.html  有Create Your Backup ROY LIN 卦嗎？？\n",
      "免費 :  https://www.ptt.cc/bbs/Tech_Job/M.1511910200.A.611.html 工作人生顧問\n",
      "ＦＢ :  https://www.ptt.cc/bbs/Tech_Job/M.1513421356.A.693.html  勞基法修法為中小企業？用數據來驗證\n",
      "ＦＢ :  https://www.ptt.cc/bbs/Tech_Job/M.1517539977.A.42E.html  勞動部:呼籲雇主不可讓勞工連續出勤逾12日\n",
      "免費 :  https://www.ptt.cc/bbs/Tech_Job/M.1476442719.A.187.html 工作人生顧問\n",
      "['討論', '心得', '請益', '面試', '聘書', '徵才', '新聞', '情報'] , 其他, 公告 (original)\n",
      "[7909, 2861, 30922, 1293, 1637, 1628, 8361, 1377]\n",
      "['討論', '心得', '請益', '情報'] , 其他, 公告\n",
      "[8598, 5983, 31379, 11656, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#整理、修改原始數據\n",
    "dfs = pd.read_csv(os.path.join(board_dir, now_time + '.csv'))\n",
    "dfs_len = len(dfs)\n",
    "\n",
    "print('Min: ', dfs.iloc[:, 5].min())\n",
    "print('0.1: ', dfs.iloc[:, 5].quantile(0.1))\n",
    "print('0.25: ', dfs.iloc[:, 5].quantile(0.25))\n",
    "print('Mean: ', dfs.iloc[:, 5].mean())\n",
    "print('0.75: ', dfs.iloc[:, 5].quantile(0.75))\n",
    "print('0.9: ', dfs.iloc[:, 5].quantile(0.9))\n",
    "print('0.95: ', dfs.iloc[:, 5].quantile(0.95))\n",
    "print('Max: ', dfs.iloc[:, 5].max())\n",
    "print('Std: ', dfs.iloc[:, 5].std())\n",
    "\n",
    "max_length = 512\n",
    "text_type = []\n",
    "text_content = []\n",
    "type_name_list = [0] * label_num\n",
    "type_newname_list = [0] * label_newnum\n",
    "for dfs_index in range(0, dfs_len):\n",
    "    read_index = dfs.ix[dfs_index, 0]\n",
    "    type_name = dfs.ix[dfs_index, 1]\n",
    "    title_name = dfs.ix[dfs_index, 9]\n",
    "    #print(read_index, ': ', dfs.ix[dfs_index, 7], title_name, end='\\r')\n",
    "    for type_num in range(label_num):\n",
    "        if type_name.find(type_label[type_num])>=0:\n",
    "            type_name_list[type_num] = type_name_list[type_num] + 1\n",
    "    \n",
    "    #排除標題與副標題\n",
    "    if type_name.find('問卷')>=0 or type_name.find('市調')>=0 or type_name.find('公告')>=0 or type_name.find('政見')>=0 or type_name.find('尋人')>=0 or type_name.find('協尋')>=0 or type_name.find('學術')>=0:\n",
    "        print('', end='\\r')\n",
    "    elif type(title_name) == type('a') and (title_name.find('問卷')>=0 or type_name.find('市調')>=0 or title_name.find('公告')>=0 or title_name.find('政見')>=0 or title_name.find('尋人')>=0 or title_name.find('協尋')>=0):\n",
    "        print('', end='\\r')\n",
    "    #整理副標題\n",
    "    elif type_name.find('聘書')>=0 or type_name.find('offer')>=0 or type_name.find('Offer')>=0 or type_name.find('OFFER')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '心得' #聘書'\n",
    "    elif type_name.find('徵才')>=0 or type_name.find('徵求')>=0 or type_name.find('求才')>=0 or type_name.find('招募')>=0 or type_name.find('誠徵')>=0 or type_name.find('急徵')>=0 or type_name.find('代徵')>=0 or type_name.find('職缺')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '情報' #'徵才'\n",
    "    elif type_name.find('台北')>=0 or type_name.find('新北')>=0 or type_name.find('雙北')>=0 or type_name.find('北部')>=0 or type_name.find('中部')>=0 or type_name.find('南部')>=0 or type_name.find('新竹')>=0 or type_name.find('北美')>=0 or type_name.find('新加坡')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '情報' #'徵才'\n",
    "    elif type_name.find('心得')>=0 or type_name.find('心情')>=0 or type_name.find('感想')>=0 or type_name.find('有感')>=0 or type_name.find('血淚史')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '心得'\n",
    "    elif type_name.find('面試')>=0 or type_name.find('面談')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '心得' #'面試'\n",
    "    elif type_name.find('新聞')>=0 or type_name.find('專欄')>=0 or type_name.find('辛聞')>=0 or type_name.find('社論')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '情報' #'新聞'\n",
    "    elif type_name.find('請益')>=0 or type_name.find('解籤')>=0 or type_name.find('問題')>=0 or type_name.find('問卦')>=0 or type_name.find('請教')>=0 or type_name.find('詢問')>=0 or type_name.find('求助')>=0 or type_name.find('疑問')>=0 or type_name.find('提問')>=0 or type_name.find('請問')>=0 or type_name.find('請求')>=0 or type_name.find('協助')>=0 or type_name.find('問題')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '請益'\n",
    "    elif type_name.find('情報')>=0 or type_name.find('開獎')>=0 or type_name.find('資訊')>=0 or type_name.find('分享')>=0 or type_name.find('活動')>=0 or type_name.find('講座')>=0 or type_name.find('職訓')>=0 or type_name.find('課程')>=0 or type_name.find('座談會')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '情報'\n",
    "    elif type_name.find('討論')>=0 or type_name.find('見鬼')>=0 or type_name.find('夢境')>=0 or type_name.find('爆卦')>=0  or type_name.find('爆掛')>=0 or type_name.find('閒聊')>=0 or type_name.find('聊天')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '討論'\n",
    "    #整理標題\n",
    "    elif type(title_name) != type('a'):\n",
    "        continue\n",
    "    elif title_name.find('聘書')>=0 or title_name.find('offer')>=0 or title_name.find('Offer')>=0 or title_name.find('OFFER')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '心得' #'聘書'\n",
    "    elif title_name.find('徵才')>=0 or title_name.find('徵求')>=0 or title_name.find('求才')>=0 or title_name.find('招募')>=0  or title_name.find('誠徵')>=0 or title_name.find('急徵')>=0 or title_name.find('代徵')>=0 or title_name.find('職缺')>=0 or title_name.find('高薪徵')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '情報' #'徵才'\n",
    "    elif title_name.find('心得')>=0 or title_name.find('心情')>=0 or title_name.find('感想')>=0 or title_name.find('有感')>=0 or title_name.find('血淚史')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '心得'\n",
    "    elif title_name.find('面試')>=0 or title_name.find('面談')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '心得' #'面試'\n",
    "    elif title_name.find('新聞')>=0 or title_name.find('專欄')>=0 or title_name.find('辛聞')>=0 or title_name.find('社論')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '情報' #'新聞'\n",
    "    elif title_name.find('請益')>=0 or title_name.find('解籤')>=0 or title_name.find('問題')>=0 or title_name.find('問卦')>=0 or title_name.find('請教')>=0 or title_name.find('詢問')>=0 or title_name.find('求助')>=0 or title_name.find('疑問')>=0 or title_name.find('提問')>=0 or title_name.find('請問')>=0 or title_name.find('請求')>=0 or title_name.find('協助')>=0 or title_name.find('問題')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '請益'\n",
    "    elif title_name.find('情報')>=0 or title_name.find('開獎')>=0 or title_name.find('分享')>=0 or title_name.find('活動')>=0 or title_name.find('講座')>=0 or title_name.find('職訓')>=0 or title_name.find('課程')>=0 or title_name.find('座談會')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '情報'\n",
    "    elif title_name.find('討論')>=0 or title_name.find('見鬼')>=0 or title_name.find('夢境')>=0 or title_name.find('爆卦')>=0 or title_name.find('爆掛')>=0 or title_name.find('閒聊')>=0 or title_name.find('聊天')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '討論'\n",
    "    #最後分類\n",
    "    elif type_name.find('無題')>=0 or type_name.find('其他')>=0:\n",
    "        dfs.ix[dfs_index, 1] = '討論'\n",
    "    else:\n",
    "        print(type_name, ': ', dfs.ix[dfs_index, 7], title_name)\n",
    "    \n",
    "    type_newname = dfs.ix[dfs_index, 1]\n",
    "    for type_num in range(label_newnum):\n",
    "        if type_newname.find(type_newlabel[type_num])>=0:\n",
    "            type_newname_list[type_num] = type_newname_list[type_num] + 1\n",
    "\n",
    "print(type_label,', 其他, 公告 (original)')\n",
    "print(type_name_list)\n",
    "print(type_newlabel,', 其他, 公告')\n",
    "print(type_newname_list)\n",
    "\n",
    "dfs.to_csv(os.path.join(board_dir, now_time + '_fix.csv'), encoding = 'utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47991\n",
      "['討論', '心得', '請益', '情報']\n",
      "[7071, 4816, 25358, 10746]\n"
     ]
    }
   ],
   "source": [
    "#Select 'type' of posts to save.\n",
    "dfs = pd.read_csv(os.path.join(board_dir, now_time + '_fix.csv'))\n",
    "word_num_min = 100\n",
    "word_num_max = 2048\n",
    "text_content = []\n",
    "for dfs_index in range(0, len(dfs)):\n",
    "    #Remove the post if it's word number is too large or small.\n",
    "    word_num = int(dfs.ix[dfs_index, 5])\n",
    "    push_num = int(dfs.ix[dfs_index, 6])\n",
    "    if word_num<word_num_min or word_num>word_num_max:\n",
    "        dfs.drop([dfs_index], inplace=True)\n",
    "        continue\n",
    "    #Remove the post if the type is not needed.\n",
    "    is_label = False\n",
    "    this_label = dfs.ix[dfs_index, 1]\n",
    "    for find_label in type_newlabel:\n",
    "        if this_label.find(find_label)>=0:\n",
    "            is_label = True\n",
    "            break\n",
    "    #Remove the post if it's no content.\n",
    "    if is_label:\n",
    "        try:\n",
    "            file = open(os.path.join(content_dir, str(dfs_index) + '.csv'), 'r', encoding = 'utf-8-sig')\n",
    "            csvCursor = csv.reader(file)\n",
    "        except BaseException:\n",
    "            is_label = False\n",
    "    if is_label:\n",
    "        text = ''\n",
    "        for row in csvCursor:\n",
    "            try:\n",
    "                text = text + row[0]\n",
    "            except BaseException:\n",
    "                continue\n",
    "        text_content.append(text[0:word_num_max])\n",
    "        file.close()\n",
    "    else: #Remove the row if it's no need.\n",
    "        dfs.drop([dfs_index], inplace=True)\n",
    "\n",
    "dfs.to_csv(os.path.join(board_dir, now_time + '_pandas.csv'), encoding = 'utf-8-sig', index=False)\n",
    "\n",
    "dfs_len = len(dfs)\n",
    "print(dfs_len)\n",
    "print(type_newlabel)\n",
    "type_newname_list = [0] * label_newnum\n",
    "for dfs_index in range(0, dfs_len):\n",
    "    for type_num in range(0, label_newnum):\n",
    "        if type_newlabel[type_num] == dfs.iloc[dfs_index, 1]:\n",
    "            type_newname_list[type_num] = type_newname_list[type_num] + 1\n",
    "\n",
    "print(type_newname_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1983,
     "status": "ok",
     "timestamp": 1517421478061,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "XLMfGvYgCdF6",
    "outputId": "46e00917-e97b-4ae8-a609-6c808458e91b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "Building prefix dict from C:\\Users\\User\\Anaconda3\\Lib\\site-packages\\jieba\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.ufa6ae29b0cbce8b45e006c7fa30eaaf8.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  100\n",
      "0.1:  128.0\n",
      "0.25:  188.0\n",
      "Mean:  511.8679127336376\n",
      "0.75:  713.0\n",
      "0.9:  1095.0\n",
      "0.95:  1364.0\n",
      "Max:  2048\n",
      "Std:  405.7673342208644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.192 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab number:  205227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:77: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy limit number:  137\n",
      "0 ('放在', 0.6837956161701394) 578\n",
      "1 ('PTT', 0.68375532376887249) 709\n",
      "2 ('脫離', 0.68370896796534719) 189\n",
      "3 ('不少', 0.68365888523370166) 2354\n",
      "4 ('以上', 0.68362222125712624) 7883\n",
      "5 ('提早', 0.68358267698926711) 474\n",
      "6 ('設定', 0.68355606106331046) 460\n",
      "7 ('35', 0.68351044299716146) 1034\n",
      "8 ('同樣', 0.68348796927123734) 1257\n",
      "9 ('的確', 0.68346191497158904) 974\n",
      "10 ('時', 0.68342097592672224) 8906\n",
      "11 ('資深', 0.68339733813153913) 2081\n",
      "12 ('做法', 0.68338506778377428) 410\n",
      "13 ('重要', 0.68338349581380142) 4292\n",
      "14 ('看似', 0.68333432415464568) 200\n",
      "15 ('包', 0.68325458178998932) 712\n",
      "16 ('唯一', 0.68322490693547144) 890\n",
      "17 ('而言', 0.68319704511984303) 888\n",
      "18 ('能力', 0.68317913789232065) 6983\n",
      "19 ('強迫', 0.6831787775967868) 293\n",
      "1000 ('維持', 0.59949360481600955) 1140\n",
      "2000 ('產線', 0.67605255030613798) 1080\n",
      "3000 ('2330', 0.44066498953645394) 320\n",
      "4000 ('QA', 0.66851385353098558) 419\n",
      "5000 ('layout', 0.61441875795228085) 362\n",
      "6000 ('紫光', 0.43749079554499776) 451\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import jieba\n",
    "\n",
    "jieba.set_dictionary('C:/Users/User/Anaconda3/Lib/site-packages/jieba/dict.txt.big')\n",
    "dfs_pandas = pd.read_csv(os.path.join(board_dir, now_time + '_pandas.csv'))\n",
    "dfs_len = len(dfs_pandas)\n",
    "\n",
    "print('Min: ', dfs_pandas.iloc[:, 5].min())\n",
    "print('0.1: ', dfs_pandas.iloc[:, 5].quantile(0.1))\n",
    "print('0.25: ', dfs_pandas.iloc[:, 5].quantile(0.25))\n",
    "print('Mean: ', dfs_pandas.iloc[:, 5].mean())\n",
    "print('0.75: ', dfs_pandas.iloc[:, 5].quantile(0.75))\n",
    "print('0.9: ', dfs_pandas.iloc[:, 5].quantile(0.9))\n",
    "print('0.95: ', dfs_pandas.iloc[:, 5].quantile(0.95))\n",
    "print('Max: ', dfs_pandas.iloc[:, 5].max())\n",
    "print('Std: ', dfs_pandas.iloc[:, 5].std())\n",
    "\n",
    "max_length = 1024\n",
    "text_type = []\n",
    "text_content = []\n",
    "dict_list = []\n",
    "type_words = [0] * label_newnum\n",
    "dict_entropy = {}\n",
    "for dict_ind in range(label_newnum):\n",
    "    dict_list.append({})\n",
    "    \n",
    "for dfs_index in range(0, dfs_len):\n",
    "    read_index = dfs_pandas.ix[dfs_index, 0]\n",
    "    read_type = type2number(dfs_pandas.ix[dfs_index, 1])\n",
    "    file = open(os.path.join(content_dir, str(read_index) + '.csv'), 'r', encoding = 'utf-8-sig')\n",
    "    csvCursor = csv.reader(file)\n",
    "    text = ''\n",
    "    for row in csvCursor:\n",
    "        try:\n",
    "            #Remove http.\n",
    "            while row[0].find('http')>=0:\n",
    "                http_str=row[0].find('http')\n",
    "                http_end=row[0].find('\\n', http_str)\n",
    "                row[0] = row[0].replace(row[0][http_str:http_end], '')\n",
    "            text = text + row[0]\n",
    "        except BaseException:\n",
    "                continue\n",
    "    text.encode('utf-8-sig')\n",
    "    text_jieba = jieba.cut(text, cut_all=False)\n",
    "    for word in text_jieba:\n",
    "        if symbol_filter(word):\n",
    "            continue\n",
    "        type_words[read_type] = type_words[read_type] + 1\n",
    "        if word not in dict_entropy.keys():\n",
    "            dict_entropy[word] = 0.0\n",
    "            for label_ind in range(label_newnum):\n",
    "                dict_list[label_ind][word] = int(label_ind == read_type)\n",
    "        else:\n",
    "            dict_list[read_type][word] = dict_list[read_type][word] + 1\n",
    "    text_content.append(text_jieba)\n",
    "    text_type.append(read_type)\n",
    "    file.close()\n",
    "\n",
    "print('Vocab number: ',len(dict_entropy))\n",
    "energy_limit = 6000\n",
    "dict_energy = {}\n",
    "type_rate = [0.0] * label_newnum\n",
    "value_sum = sum(type_words)\n",
    "for label_ind in range(label_newnum):\n",
    "    type_rate[label_ind] = type_words[label_ind] / value_sum\n",
    "\n",
    "for dict_word in dict_entropy:\n",
    "    value_sum = 0\n",
    "    for label_ind in range(label_newnum):\n",
    "        value_sum = value_sum + dict_list[label_ind][dict_word]\n",
    "    dict_energy[dict_word] = value_sum\n",
    "    entropy = 0.0\n",
    "    for label_ind in range(label_newnum):\n",
    "        prop = type_rate[label_ind] * dict_list[label_ind][dict_word] / value_sum\n",
    "        entropy = entropy - prop * np.log(prop)\n",
    "    dict_entropy[dict_word] = entropy\n",
    "\n",
    "list_energy = sorted(dict_energy.items(), key=lambda dict_energy: dict_energy[1], reverse=True)\n",
    "energy_limit_num = list_energy[energy_limit][1]\n",
    "print('Energy limit number: ',energy_limit_num)\n",
    "\n",
    "for vocab_index in dict_energy:\n",
    "    if dict_energy[vocab_index] < energy_limit_num:\n",
    "        del dict_entropy[vocab_index]\n",
    "list_entropy = sorted(dict_entropy.items(), key=lambda dict_entropy: dict_entropy[1], reverse=True)\n",
    "\n",
    "for vocab_index in range(len(dict_entropy)):\n",
    "    if vocab_index % 1000 == 0 or vocab_index < 20:\n",
    "        print(vocab_index, list_entropy[vocab_index], dict_energy[list_entropy[vocab_index][0]])\n",
    "    vocab_index = vocab_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['OK', 'GG', 'JB'])\n",
      "dict_values([1, 0, 2])\n",
      "dict_items([('OK', 1), ('GG', 0), ('JB', 2)])\n",
      "[('JB', 2), ('OK', 1), ('GG', 0)]\n",
      "JB\n"
     ]
    }
   ],
   "source": [
    "d = {'OK':1,'GG':0,'JB':2}\n",
    "x = d.keys()\n",
    "y = d.values()\n",
    "z = d.items()\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "l = sorted(d.items(), key=lambda d: d[1], reverse=True)\n",
    "print(l)\n",
    "print(l[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab number:  198401\n",
      "vocab_index vocab_word vocab_cnt vocab_doc\n",
      "0 的 401906 45214\n",
      "1 是 134519 36778\n",
      "2 我 105585 24549\n",
      "3 有 97402 35125\n",
      "4 在 88650 31686\n",
      "5 了 80607 27791\n",
      "6 公司 78199 27894\n",
      "7 也 71346 27138\n",
      "8 都 61279 25135\n",
      "9 工作 59346 23601\n",
      "1000 看法 1413 1298\n",
      "2000 角度 615 529\n",
      "3000 ie 357 225\n",
      "4000 調漲 241 162\n",
      "5000 87 178 140\n",
      "6000 童子 139 53\n",
      "7000 潭子 111 95\n",
      "8000 再有 92 88\n",
      "9000 搭車 78 73\n",
      "10000 二哥 67 56\n",
      "11000 敢講 58 53\n",
      "12000 規章 51 38\n",
      "13000 air 45 32\n",
      "14000 睿 41 29\n",
      "15000 甄上 37 35\n",
      "16000 全力支持 33 32\n",
      "17000 政商 30 28\n",
      "18000 傳產當 28 28\n",
      "19000 損壞 26 22\n",
      "20000 當奴才 24 20\n",
      "21000 某私 22 22\n",
      "22000 郭易 21 9\n",
      "23000 有段 19 19\n",
      "24000 八折 18 17\n",
      "25000 趨向 17 16\n",
      "26000 往日 16 16\n",
      "27000 吸取 15 15\n",
      "28000 壽險業 14 9\n",
      "29000 通告 13 11\n",
      "30000 埋首 13 8\n",
      "31000 給在 12 12\n",
      "32000 殺戮 11 5\n",
      "33000 分心 11 11\n",
      "34000 四元 10 8\n",
      "35000 末位 10 7\n",
      "36000 recruitment 9 9\n",
      "37000 密不可分 9 9\n",
      "38000 禎 9 6\n",
      "39000 savage 8 8\n",
      "40000 奮力 8 8\n",
      "41000 必反 8 5\n",
      "42000 控器 7 3\n",
      "43000 半小 7 7\n",
      "44000 治病 7 7\n",
      "45000 傳送帶 7 2\n",
      "46000 河東 6 6\n",
      "47000 活過來 6 6\n",
      "48000 陳俊廷 6 1\n",
      "49000 人報 6 6\n",
      "50000 遭法 6 2\n",
      "51000 差一截 5 5\n",
      "52000 關電 5 5\n",
      "53000 地為 5 5\n",
      "54000 敗血症 5 5\n",
      "55000 dftz 5 4\n",
      "56000 張碩文 5 5\n",
      "57000 五六百 4 4\n",
      "58000 造船系 4 4\n",
      "59000 廠人資 4 4\n",
      "60000 最原 4 4\n",
      "61000 幾萬個 4 4\n",
      "62000 alberto 4 4\n",
      "63000 炒菜 4 4\n",
      "64000 越輪 4 4\n",
      "65000 金融部門 4 2\n",
      "66000 贈與稅 4 2\n",
      "67000 周星馳 3 3\n",
      "68000 laszlo 3 3\n",
      "69000 亂罵 3 3\n",
      "70000 業快 3 3\n",
      "71000 他僅 3 3\n",
      "72000 那換個 3 3\n",
      "73000 加幣 3 3\n",
      "74000 從豬 3 3\n",
      "75000 裝病 3 3\n",
      "76000 上海浦東 3 2\n",
      "77000 科締納 3 1\n",
      "78000 ninefboomer 3 3\n",
      "79000 季提列 3 2\n",
      "80000 西可德信 3 3\n",
      "81000 sigmaintell 3 3\n",
      "82000 大陸同胞 2 2\n",
      "83000 2451 2 2\n",
      "84000 囤貨 2 2\n",
      "85000 智王 2 2\n",
      "86000 逆襲 2 2\n",
      "87000 坐不燒 2 2\n",
      "88000 三十多 2 2\n",
      "89000 朝瑞 2 2\n",
      "90000 祭祖 2 2\n",
      "91000 職生 2 2\n",
      "92000 不洽 2 2\n",
      "93000 人版 2 2\n",
      "94000 痛腳 2 2\n",
      "95000 wih 2 2\n",
      "96000 人體器官 2 2\n",
      "97000 17or18 2 2\n",
      "98000 綉 2 2\n",
      "99000 支給表 2 2\n",
      "100000 朱玉書 2 2\n",
      "101000 深有感觸 2 2\n",
      "102000 金害 2 2\n",
      "103000 內弟 2 1\n",
      "104000 psos 2 1\n",
      "105000 卡泰琪安 2 1\n",
      "106000 建線 2 1\n",
      "107000 dirk147862 2 1\n",
      "108000 dane 2 2\n",
      "109000 億能 2 2\n",
      "110000 攻擊機 2 2\n",
      "111000 免進 2 2\n",
      "112000 互勵 1 1\n",
      "113000 海跟夏普 1 1\n",
      "114000 逾限 1 1\n",
      "115000 圈外人 1 1\n",
      "116000 這尊 1 1\n",
      "117000 holy 1 1\n",
      "118000 貴司還 1 1\n",
      "119000 名均 1 1\n",
      "120000 中心廣場 1 1\n",
      "121000 騙站 1 1\n",
      "122000 太誘 1 1\n",
      "123000 億鎂 1 1\n",
      "124000 中鋼工 1 1\n",
      "125000 伶俐 1 1\n",
      "126000 到觸 1 1\n",
      "127000 貴不貴 1 1\n",
      "128000 太低害 1 1\n",
      "129000 捆工 1 1\n",
      "130000 增田 1 1\n",
      "131000 激似 1 1\n",
      "132000 科通 1 1\n",
      "133000 紅黨 1 1\n",
      "134000 李宜庭 1 1\n",
      "135000 另候 1 1\n",
      "136000 costell 1 1\n",
      "137000 將炙 1 1\n",
      "138000 圓孔 1 1\n",
      "139000 且業外 1 1\n",
      "140000 震分 1 1\n",
      "141000 眾智 1 1\n",
      "142000 1jg3w0kz 1 1\n",
      "143000 1js2ky82 1 1\n",
      "144000 放鴿 1 1\n",
      "145000 solinsomn 1 1\n",
      "146000 南科太遠 1 1\n",
      "147000 人勝 1 1\n",
      "148000 還脫 1 1\n",
      "149000 themost 1 1\n",
      "150000 完水 1 1\n",
      "151000 還算聊 1 1\n",
      "152000 tenderloin 1 1\n",
      "153000 masterclass 1 1\n",
      "154000 sersa 1 1\n",
      "155000 達負 1 1\n",
      "156000 transferred 1 1\n",
      "157000 安份 1 1\n",
      "158000 civic8 1 1\n",
      "159000 排台 1 1\n",
      "160000 談延替 1 1\n",
      "161000 成可有 1 1\n",
      "162000 50kx14 1 1\n",
      "163000 四川菜 1 1\n",
      "164000 廠綠 1 1\n",
      "165000 該署 1 1\n",
      "166000 抽蠻 1 1\n",
      "167000 鄭夢九 1 1\n",
      "168000 成此波 1 1\n",
      "169000 隔行 1 1\n",
      "170000 大水庫 1 1\n",
      "171000 心肺 1 1\n",
      "172000 等議 1 1\n",
      "173000 雷克斯 1 1\n",
      "174000 15p6 1 1\n",
      "175000 不多要 1 1\n",
      "176000 很物聯 1 1\n",
      "177000 profgy 1 1\n",
      "178000 wezr 1 1\n",
      "179000 漢納 1 1\n",
      "180000 二行 1 1\n",
      "181000 業季 1 1\n",
      "182000 shangri 1 1\n",
      "183000 欽定 1 1\n",
      "184000 胡立陽 1 1\n",
      "185000 恩智浦才剛 1 1\n",
      "186000 幾還 1 1\n",
      "187000 相性 1 1\n",
      "188000 rebasing 1 1\n",
      "189000 萊迪斯 1 1\n",
      "190000 非寶 1 1\n",
      "191000 食晶圓 1 1\n",
      "192000 新開設 1 1\n",
      "193000 拋離 1 1\n",
      "194000 erc20 1 1\n",
      "195000 caffe2 1 1\n",
      "196000 競系 1 1\n",
      "197000 廠富智康 1 1\n",
      "198000 選先 1 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_content, text_type, test_size=0.2, random_state=9487)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, label_newnum)\n",
    "y_test = np_utils.to_categorical(y_test, label_newnum)\n",
    "\n",
    "# 序列模式\n",
    "x_train = pad_sequences(x_train_word_ids, maxlen=max_length)\n",
    "x_test = pad_sequences(x_test_word_ids, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1343,
     "output_extras": [
      {
       "item_id": 18
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9160,
     "status": "ok",
     "timestamp": 1517421499174,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "-iUlULwfCdGC",
    "outputId": "3a3b3192-bbe8-4534-81d5-53f2e05b9566",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 1024)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1024, 300)     2400000     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 1024, 256)     230656      embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 256, 256)      0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 65536)         0           max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 512)           855552      embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 256)           16777472    flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           131328      bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 256)           0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 256)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 512)           0           dropout_3[0][0]                  \n",
      "                                                                   dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 4)             2052        concatenate_2[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 20,397,060\n",
      "Trainable params: 20,397,060\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "\n",
      "#######\n",
      "epoch1\n",
      "#######\n",
      "Iteration  59  ( 9.67 %) Train Loss:  0.922212 ; Train accuracy:  0.671875           \n",
      "Iteration  118  ( 19.5 %) Train Loss:  0.750908 ; Train accuracy:  0.65625           \n",
      "Iteration  177  ( 29.33 %) Train Loss:  0.721127 ; Train accuracy:  0.734375           \n",
      "Iteration  236  ( 39.17 %) Train Loss:  0.889336 ; Train accuracy:  0.671875           \n",
      "Iteration  295  ( 49.0 %) Train Loss:  0.824993 ; Train accuracy:  0.640625           \n",
      "Iteration  354  ( 58.83 %) Train Loss:  0.647154 ; Train accuracy:  0.6875           \n",
      "Iteration  413  ( 68.67 %) Train Loss:  0.58796 ; Train accuracy:  0.796875           \n",
      "Iteration  472  ( 78.5 %) Train Loss:  0.432501 ; Train accuracy:  0.8125           \n",
      "Iteration  531  ( 88.33 %) Train Loss:  0.474554 ; Train accuracy:  0.75           \n",
      "Iteration  590  ( 98.17 %) Train Loss:  0.646005 ; Train accuracy:  0.796875           \n",
      "Train Loss:  0.851052 ; Train accuracy:  0.732143; Train accuracy:  0.732143\n",
      "9599/9599 [==============================] - 76s    \n",
      "Test Loss:  0.615888727296 : Test accuracy:  0.762891968379\n",
      "Save best score!! 0.762891968379\n",
      "Elapsed time in epoch 1: 1287.6145956516266\n",
      "Saved model 1!\n",
      "\n",
      "#######\n",
      "epoch2\n",
      "#######\n",
      "Iteration  59  ( 9.67 %) Train Loss:  0.518424 ; Train accuracy:  0.75           \n",
      "Iteration  118  ( 19.5 %) Train Loss:  0.318298 ; Train accuracy:  0.921875           \n",
      "Iteration  177  ( 29.33 %) Train Loss:  0.566445 ; Train accuracy:  0.765625           \n",
      "Iteration  236  ( 39.17 %) Train Loss:  0.445903 ; Train accuracy:  0.859375           \n",
      "Iteration  295  ( 49.0 %) Train Loss:  0.640976 ; Train accuracy:  0.734375           \n",
      "Iteration  354  ( 58.83 %) Train Loss:  0.442212 ; Train accuracy:  0.75           \n",
      "Iteration  413  ( 68.67 %) Train Loss:  0.558647 ; Train accuracy:  0.84375           \n",
      "Iteration  472  ( 78.5 %) Train Loss:  0.255583 ; Train accuracy:  0.90625           \n",
      "Iteration  531  ( 88.33 %) Train Loss:  0.71321 ; Train accuracy:  0.6875           \n",
      "Iteration  590  ( 98.17 %) Train Loss:  0.578734 ; Train accuracy:  0.78125           \n",
      "Train Loss:  0.415221 ; Train accuracy:  0.839286; Train accuracy:  0.839286\n",
      "9599/9599 [==============================] - 75s    \n",
      "Test Loss:  0.660009018045 : Test accuracy:  0.760391707929\n",
      "Elapsed time in epoch 2: 1280.2439739704132\n",
      "Saved model 2!\n",
      "\n",
      "#######\n",
      "epoch3\n",
      "#######\n",
      "Iteration  59  ( 9.67 %) Train Loss:  0.194147 ; Train accuracy:  0.921875           \n",
      "Iteration  118  ( 19.5 %) Train Loss:  0.165021 ; Train accuracy:  0.9375           \n",
      "Iteration  177  ( 29.33 %) Train Loss:  0.30642 ; Train accuracy:  0.90625           \n",
      "Iteration  236  ( 39.17 %) Train Loss:  0.271306 ; Train accuracy:  0.890625           \n",
      "Iteration  295  ( 49.0 %) Train Loss:  0.196577 ; Train accuracy:  0.9375           \n",
      "Iteration  354  ( 58.83 %) Train Loss:  0.467425 ; Train accuracy:  0.890625           \n",
      "Iteration  413  ( 68.67 %) Train Loss:  0.150845 ; Train accuracy:  0.9375           \n",
      "Iteration  472  ( 78.5 %) Train Loss:  0.197392 ; Train accuracy:  0.921875           \n",
      "Iteration  531  ( 88.33 %) Train Loss:  0.210206 ; Train accuracy:  0.921875           \n",
      "Iteration  590  ( 98.17 %) Train Loss:  0.24948 ; Train accuracy:  0.890625           \n",
      "Train Loss:  0.347535 ; Train accuracy:  0.87535 ; Train accuracy:  0.875255\n",
      "9599/9599 [==============================] - 75s    \n",
      "Test Loss:  0.97285504861 : Test accuracy:  0.756849672325\n",
      "Elapsed time in epoch 3: 1267.8093984127045\n",
      "Saved model 3!\n",
      "Stop by early stopping\n",
      "Best score: 0.762891968379\n",
      "Elapsed time in total: 3837.39213180542\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Concatenate\n",
    "from keras.layers import Conv1D, Flatten, Dropout, MaxPool1D\n",
    "from keras.layers import LSTM, GRU, TimeDistributed, Bidirectional\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "import keras.callbacks\n",
    "\n",
    "patience = 2\n",
    "num_epoch = 12\n",
    "batch_size = 64\n",
    "save_every = 1\n",
    "pretrain = 0\n",
    "\n",
    "model_name = 'RNN_PTT_model_fixed_' + str(time.time())\n",
    "mdl_dir = os.path.join(base_dir, 'model')\n",
    "if not os.path.exists(mdl_dir):\n",
    "    os.makedirs(mdl_dir)\n",
    "model_dir = os.path.join(mdl_dir, model_name)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "def build_model():\n",
    "    main_input = Input(shape=(max_length,), dtype='float64')\n",
    "    \n",
    "    embed = Embedding(8000, 300, input_length=max_length)(main_input)\n",
    "    \n",
    "    cnn = Conv1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
    "    cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "    cnn = Flatten()(cnn)\n",
    "    cnn = Dense(256)(cnn)\n",
    "    cnn = Dropout(0.5)(cnn)\n",
    "    \n",
    "    rnn = Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1))(embed)\n",
    "    rnn = Dense(256)(rnn)\n",
    "    rnn = Dropout(0.5)(rnn)\n",
    "    \n",
    "    con = Concatenate(axis=-1)([cnn,rnn])\n",
    "\n",
    "    main_output  = Dense(label_newnum, activation = 'softmax')(con)\n",
    "    \n",
    "    model = Model(inputs = main_input, outputs = main_output)\n",
    "    #opt = Adadelta(lr = 0.05, rho = 0.95, epsilon = 1e-06)\n",
    "    opt = 'adam'\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=opt, metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train():\n",
    "    if pretrain == 0:\n",
    "        model = build_model()\n",
    "    else:\n",
    "        model = load_model(model_name)\n",
    "            \n",
    "    num_instances = len(y_train)\n",
    "    iter_per_epoch = int(num_instances / batch_size)\n",
    "    iter_pct10 = int(iter_per_epoch / 10)\n",
    "    if (num_instances % batch_size) > 0:\n",
    "        iter_per_epoch = iter_per_epoch + 1\n",
    "    batch_cutoff = [0]\n",
    "    for i in range(iter_per_epoch - 1):\n",
    "        batch_cutoff.append(batch_size * (i+1))\n",
    "    batch_cutoff.append(num_instances)\n",
    "    \n",
    "    total_start_t = time.time()\n",
    "    best_metrics = 0.0\n",
    "    early_stop_counter = 0\n",
    "    for e in range(num_epoch):\n",
    "        rand_idxs = np.random.permutation(num_instances)\n",
    "        print('\\n#######')\n",
    "        print('epoch' + str(e+1))\n",
    "        print('#######')\n",
    "        start_t = time.time()\n",
    "        for i in range(iter_per_epoch):\n",
    "            X_batch = []\n",
    "            Y_batch = []\n",
    "            \n",
    "            for n in range(batch_cutoff[i],batch_cutoff[i+1]):\n",
    "                X_batch.append(x_train[rand_idxs[n]])\n",
    "                Y_batch.append(y_train[rand_idxs[n]])\n",
    "            loss_and_metrics = model.train_on_batch(np.asarray(X_batch),np.asarray(Y_batch))\n",
    "            if i % iter_pct10 == iter_pct10 - 1:\n",
    "                print('Iteration ',i+1,' (',round(i*100/iter_per_epoch,2),'%) Train Loss: ',loss_and_metrics[0],'; Train accuracy: ',loss_and_metrics[1],'              ')\n",
    "            else:\n",
    "                print('Iteration ',i+1,' (',round(i*100/iter_per_epoch,2),'%) Train Loss: ',loss_and_metrics[0],'; Train accuracy: ',loss_and_metrics[1],'              ',end='\\r')\n",
    "                \n",
    "        print('Train Loss: ',loss_and_metrics[0],'; Train accuracy: ',loss_and_metrics[1])\n",
    "        loss_and_metrics = model.evaluate(x_test, y_test, batch_size)\n",
    "        print('Test Loss: ',loss_and_metrics[0],': Test accuracy: ',loss_and_metrics[1])\n",
    "        \n",
    "        if loss_and_metrics[1] >= best_metrics:\n",
    "            best_metrics = loss_and_metrics[1]\n",
    "            print('Save best score!! '+str(loss_and_metrics[1]))\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            \n",
    "        print('Elapsed time in epoch ' + str(e+1) + ': ' + str(time.time() - start_t))\n",
    "        \n",
    "        if (e+1) % save_every == 0:\n",
    "            model_path = os.path.join(model_dir, 'model-%d.h5' %(e+1))\n",
    "            model.save(model_path)\n",
    "            model_weights_path = os.path.join(model_dir, 'model_weights-%d.h5' %(e+1))\n",
    "            model.save_weights(model_weights_path)\n",
    "            print('Saved model %s!' %str(e+1))\n",
    "            \n",
    "        if early_stop_counter >= patience:\n",
    "            print('Stop by early stopping')\n",
    "            print('Best score: ' + str(best_metrics))\n",
    "            break\n",
    "            \n",
    "    print('Elapsed time in total: ' + str(time.time() - total_start_t))\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "MNIST Model.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
